
Traceback (most recent call last):
  File "evaluate_agents.py", line 213, in <module>
    evaluate_sample_eff(agent, env, env_eval, seeds, total_timesteps, evaluation_freq, n_episodes_to_evaluate, save_dir)
  File "evaluate_agents.py", line 103, in evaluate_sample_eff
    agent.update(curr_obs, action, reward, next_obs, done, timestep)
  File "XXX/COMP579_project/GROUPS/GROUP_056/agent.py", line 103, in update
    self.train(self.memory, timestep, self.mini_batch_size)
  File "XXX/COMP579_project/GROUPS/GROUP_056/agent.py", line 138, in train
    policy_loss.backward()
  File "XXX/.conda/envs/COMP579_project/lib/python3.7/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "XXX/.conda/envs/COMP579_project/lib/python3.7/site-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [256, 1]], which is output 0 of AsStridedBackward0, is at version 3; expected version 2 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
